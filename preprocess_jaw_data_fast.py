#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PointAttN ç‰™åˆ—æ•°æ®é¢„å¤„ç†è„šæœ¬ - å¿«é€Ÿç‰ˆæœ¬
å»é™¤å¤æ‚çš„ç½‘æ ¼æ“ä½œï¼Œä¸“æ³¨äºå¿«é€Ÿå¤„ç†
"""

import os
import json
import numpy as np
from tqdm import tqdm
import random
from pathlib import Path
import multiprocessing as mp
from functools import partial
import time
from excluded_samples import EXCLUDED_SAMPLE_IDS, is_sample_excluded

# é…ç½®å‚æ•°
RAW_DATA_DIR = "data_prepare/é‡æ ‡æ³¨åå®Œæ•´ç‰™åˆ—lower_jaw"
OUTPUT_DIR = "processed_jaw_data"
N_POINTS = 2048
TRAIN_RATIO = 0.8
TEST_RATIO = 0.2
N_PARTIAL_VIEWS = 8

# æ€§èƒ½ä¼˜åŒ–é…ç½®
NUM_PROCESSES = min(mp.cpu_count(), 16)  # ä½¿ç”¨å¤šè¿›ç¨‹
CHUNK_SIZE = 10  # æ¯ä¸ªè¿›ç¨‹å¤„ç†çš„æ ·æœ¬æ•°

def ensure_dir(path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    Path(path).mkdir(parents=True, exist_ok=True)

def read_obj_fast(obj_path):
    """å¿«é€Ÿè¯»å–OBJæ–‡ä»¶çš„é¡¶ç‚¹"""
    vertices = []
    try:
        with open(obj_path, 'r') as f:
            for line in f:
                if line.startswith('v '):
                    parts = line.strip().split()
                    if len(parts) >= 4:
                        vertices.append([float(parts[1]), float(parts[2]), float(parts[3])])
        return np.array(vertices) if vertices else None
    except Exception as e:
        print(f"Error reading {obj_path}: {e}")
        return None

def read_labels_fast(json_path):
    """å¿«é€Ÿè¯»å–æ ‡ç­¾"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            label_data = json.load(f)

        labels = []
        for label in label_data['labels']:
            if isinstance(label, (int, float)):
                labels.append(int(label))
            elif isinstance(label, str) and label.isdigit():
                labels.append(int(label))
            else:
                labels.append(0)

        return np.array(labels)
    except Exception as e:
        print(f"Error reading {json_path}: {e}")
        return None

def normalize_vertices(vertices):
    """å¿«é€Ÿå½’ä¸€åŒ–é¡¶ç‚¹"""
    bbox_min = vertices.min(axis=0)
    bbox_max = vertices.max(axis=0)
    center = (bbox_min + bbox_max) / 2
    scale = np.max(bbox_max - bbox_min) / 2
    return (vertices - center) / scale

def sample_points_fast(vertices, n_points):
    """å¿«é€Ÿç‚¹é‡‡æ ·"""
    if len(vertices) >= n_points:
        indices = np.random.choice(len(vertices), n_points, replace=False)
    else:
        indices = np.random.choice(len(vertices), n_points, replace=True)
    return vertices[indices]

def create_partial_fast(vertices, strategy='z_split', **kwargs):
    """å¿«é€Ÿåˆ›å»ºpartialç‚¹äº‘"""
    if strategy == 'z_split':
        mask_ratio = kwargs.get('mask_ratio', 0.5)
        z_min = vertices[:, 2].min()
        z_max = vertices[:, 2].max()
        z_threshold = z_min + (z_max - z_min) * mask_ratio
        mask = vertices[:, 2] > z_threshold

        # ç¡®ä¿è‡³å°‘æœ‰ä¸€äº›ç‚¹
        if np.sum(mask) < 100:
            mask = vertices[:, 2] > (z_min + (z_max - z_min) * 0.3)

        return vertices[mask]

    elif strategy == 'random_remove':
        keep_ratio = kwargs.get('keep_ratio', 0.7)
        n_keep = max(int(len(vertices) * keep_ratio), 100)
        indices = np.random.choice(len(vertices), n_keep, replace=False)
        return vertices[indices]

    else:
        # é»˜è®¤éšæœºé‡‡æ ·
        n_keep = max(int(len(vertices) * 0.7), 100)
        indices = np.random.choice(len(vertices), n_keep, replace=False)
        return vertices[indices]

def save_points_simple(points, filepath):
    """ç®€å•å¿«é€Ÿçš„ç‚¹äº‘ä¿å­˜ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰"""
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filepath), exist_ok=True)

        # ä¿å­˜ä¸ºç®€å•çš„æ–‡æœ¬æ ¼å¼ï¼ˆæ›´å¿«ï¼‰
        np.savetxt(filepath.replace('.pcd', '.txt'), points, fmt='%.6f')

        # å¦‚æœéœ€è¦PCDæ ¼å¼ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„PCDæ–‡ä»¶
        with open(filepath, 'w') as f:
            f.write("# .PCD v0.7 - Point Cloud Data file format\n")
            f.write("VERSION 0.7\n")
            f.write("FIELDS x y z\n")
            f.write("SIZE 4 4 4\n")
            f.write("TYPE F F F\n")
            f.write("COUNT 1 1 1\n")
            f.write(f"WIDTH {len(points)}\n")
            f.write("HEIGHT 1\n")
            f.write("VIEWPOINT 0 0 0 1 0 0 0\n")
            f.write(f"POINTS {len(points)}\n")
            f.write("DATA ascii\n")
            for point in points:
                f.write(f"{point[0]:.6f} {point[1]:.6f} {point[2]:.6f}\n")

        return True
    except Exception as e:
        print(f"Error saving {filepath}: {e}")
        return False

def process_single_sample_fast(args):
    """å¿«é€Ÿå¤„ç†å•ä¸ªæ ·æœ¬"""
    sample_path, instance_id, output_dir, split = args

    try:
        # è¯»å–æ•°æ®
        obj_path = os.path.join(sample_path, "lower.obj")
        json_path = os.path.join(sample_path, "modified_seg.json")

        vertices = read_obj_fast(obj_path)
        labels = read_labels_fast(json_path)

        if vertices is None or labels is None:
            return None

        # å¿«é€Ÿå½’ä¸€åŒ–
        vertices = normalize_vertices(vertices)

        # ç”Ÿæˆå®Œæ•´ç‚¹äº‘
        complete_points = sample_points_fast(vertices, N_POINTS)

        # ä¿å­˜å®Œæ•´ç‚¹äº‘
        complete_dir = os.path.join(output_dir, split, "complete", "000")
        complete_path = os.path.join(complete_dir, f"{instance_id}.pcd")
        save_points_simple(complete_points, complete_path)

        # ç”Ÿæˆpartialç‚¹äº‘
        partial_dir = os.path.join(output_dir, split, "partial", "000", instance_id)

        for i in range(N_PARTIAL_VIEWS):
            # å¿«é€Ÿç”Ÿæˆpartial
            if random.random() < 0.5:
                partial_vertices = create_partial_fast(
                    vertices, 'z_split',
                    mask_ratio=random.uniform(0.3, 0.7)
                )
            else:
                partial_vertices = create_partial_fast(
                    vertices, 'random_remove',
                    keep_ratio=random.uniform(0.5, 0.8)
                )

            # é‡‡æ ·partialç‚¹äº‘
            partial_points = sample_points_fast(partial_vertices, N_POINTS // 4)

            # ä¿å­˜partialç‚¹äº‘
            partial_path = os.path.join(partial_dir, f"{i:02d}.pcd")
            save_points_simple(partial_points, partial_path)

        return instance_id

    except Exception as e:
        print(f"Error processing {instance_id}: {e}")
        return None

def collect_all_samples():
    """æ”¶é›†æ‰€æœ‰æ ·æœ¬ä¿¡æ¯"""
    samples = []
    excluded_count = 0
    total_found = 0

    print("ğŸ“‹ æ‰«ææ•°æ®ç›®å½•...")

    for jaw_dir in ["lower_jaw1", "lower_jaw2", "lower_jaw3"]:
        jaw_path = os.path.join(RAW_DATA_DIR, jaw_dir)
        if not os.path.exists(jaw_path):
            continue

        jaw_samples = []
        for instance_id in os.listdir(jaw_path):
            instance_path = os.path.join(jaw_path, instance_id)
            if os.path.isdir(instance_path):
                obj_path = os.path.join(instance_path, "lower.obj")
                json_path = os.path.join(instance_path, "modified_seg.json")

                if os.path.exists(obj_path) and os.path.exists(json_path):
                    total_found += 1

                    if is_sample_excluded(instance_id):
                        excluded_count += 1
                        continue

                    jaw_samples.append((instance_path, instance_id))

        samples.extend(jaw_samples)
        print(f"   âœ… {jaw_dir}: {len(jaw_samples)} æœ‰æ•ˆæ ·æœ¬")

    print(f"ğŸ“Š æ€»ç»Ÿè®¡: {len(samples)} æœ‰æ•ˆæ ·æœ¬ / {excluded_count} æ’é™¤ / {total_found} æ€»è®¡")
    return samples

def create_dataset_metadata(output_dir, train_samples, test_samples):
    """åˆ›å»ºæ•°æ®é›†å…ƒæ•°æ®æ–‡ä»¶"""
    dataset_json = [
        {
            "taxonomy_id": "000",
            "taxonomy_name": "LowerJaw",
            "test": test_samples,
            "train": train_samples,
            "val": []
        }
    ]

    with open(os.path.join(output_dir, "PCN.json"), 'w') as f:
        json.dump(dataset_json, f, indent=2)

    with open(os.path.join(output_dir, "category.txt"), 'w') as f:
        f.write("LowerJaw\n")

def main():
    """ä¸»å¤„ç†å‡½æ•° - å¿«é€Ÿç‰ˆæœ¬"""
    start_time = time.time()

    print("ğŸš€ å¼€å§‹å¿«é€Ÿç‰™åˆ—æ•°æ®é¢„å¤„ç†...")
    print(f"âš¡ ä½¿ç”¨ {NUM_PROCESSES} ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    ensure_dir(OUTPUT_DIR)
    for split in ['train', 'test']:
        ensure_dir(os.path.join(OUTPUT_DIR, split, 'complete', '000'))
        ensure_dir(os.path.join(OUTPUT_DIR, split, 'partial', '000'))

    # æ”¶é›†æ ·æœ¬
    all_samples = collect_all_samples()

    if len(all_samples) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ ·æœ¬ï¼")
        return

    # åˆ†å‰²æ•°æ®
    random.shuffle(all_samples)
    n_train = int(len(all_samples) * TRAIN_RATIO)
    train_samples = all_samples[:n_train]
    test_samples = all_samples[n_train:]

    print(f"ğŸ“Š æ•°æ®åˆ†å‰²: {len(train_samples)} è®­ç»ƒ + {len(test_samples)} æµ‹è¯•")

    # å‡†å¤‡ä»»åŠ¡
    train_tasks = [(sample_path, instance_id, OUTPUT_DIR, "train")
                   for sample_path, instance_id in train_samples]
    test_tasks = [(sample_path, instance_id, OUTPUT_DIR, "test")
                  for sample_path, instance_id in test_samples]
    all_tasks = train_tasks + test_tasks

    print(f"âš¡ å¼€å§‹å¹¶è¡Œå¤„ç† {len(all_tasks)} ä¸ªæ ·æœ¬...")

    # å¤šè¿›ç¨‹å¤„ç†
    processed_train = []
    processed_test = []

    with mp.Pool(NUM_PROCESSES) as pool:
        with tqdm(total=len(all_tasks), desc="å¤„ç†æ ·æœ¬", unit="samples") as pbar:
            # ä½¿ç”¨ imap_unordered è·å¾—æ›´å¥½çš„æ€§èƒ½
            results = pool.imap_unordered(process_single_sample_fast, all_tasks, chunksize=CHUNK_SIZE)

            for i, result in enumerate(results):
                if result is not None:
                    if i < len(train_tasks):
                        processed_train.append(result)
                    else:
                        processed_test.append(result)
                pbar.update(1)

    # åˆ›å»ºå…ƒæ•°æ®
    print("ğŸ“ åˆ›å»ºæ•°æ®é›†å…ƒæ•°æ®...")
    create_dataset_metadata(OUTPUT_DIR, processed_train, processed_test)

    end_time = time.time()
    total_time = end_time - start_time

    print(f"\nğŸ‰ å¿«é€Ÿé¢„å¤„ç†å®Œæˆï¼")
    print(f"â±ï¸  æ€»å¤„ç†æ—¶é—´: {total_time:.2f} ç§’")
    print(f"ğŸ“Š å¤„ç†é€Ÿåº¦: {len(all_samples)/total_time:.2f} æ ·æœ¬/ç§’")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"âœ… æˆåŠŸå¤„ç†: {len(processed_train)} è®­ç»ƒ + {len(processed_test)} æµ‹è¯•")

    # æ˜¾ç¤ºæ€§èƒ½æå‡
    estimated_original_time = len(all_samples) * 5  # å‡è®¾åŸç‰ˆæœ¬æ¯æ ·æœ¬5ç§’
    speedup = estimated_original_time / total_time
    print(f"ğŸš€ é¢„ä¼°åŠ é€Ÿæ¯”: {speedup:.1f}x")

if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    np.random.seed(42)

    main()