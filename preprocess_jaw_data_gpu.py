#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PointAttN ç‰™åˆ—æ•°æ®é¢„å¤„ç†è„šæœ¬ - GPUåŠ é€Ÿç‰ˆæœ¬
ä¼˜åŒ–äº†å¤„ç†é€Ÿåº¦ï¼Œå……åˆ†åˆ©ç”¨GPUå’Œå¹¶è¡Œå¤„ç†
"""

import os
import json
import trimesh
import numpy as np
import open3d as o3d
from tqdm import tqdm
import random
import shutil
from pathlib import Path
import multiprocessing
from functools import partial
import torch
import time
from excluded_samples import EXCLUDED_SAMPLE_IDS, is_sample_excluded

# é…ç½®å‚æ•°
RAW_DATA_DIR = "data_prepare/é‡æ ‡æ³¨åå®Œæ•´ç‰™åˆ—lower_jaw"
OUTPUT_DIR = "processed_jaw_data"
N_POINTS = 2048
TRAIN_RATIO = 0.8
TEST_RATIO = 0.2
N_PARTIAL_VIEWS = 8

# GPUåŠ é€Ÿé…ç½®
USE_GPU = torch.cuda.is_available()
BATCH_SIZE = 16  # æ‰¹é‡å¤„ç†å¤§å°
DEVICE = torch.device('cuda' if USE_GPU else 'cpu')

print(f"ğŸš€ GPUåŠ é€Ÿ: {'å¯ç”¨' if USE_GPU else 'ç¦ç”¨'} (è®¾å¤‡: {DEVICE})")

def ensure_dir(path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    Path(path).mkdir(parents=True, exist_ok=True)

def gpu_sample_points(vertices, n_points, device=DEVICE):
    """
    ä½¿ç”¨GPUåŠ é€Ÿçš„ç‚¹äº‘é‡‡æ ·
    """
    if not USE_GPU:
        # å›é€€åˆ°CPUé‡‡æ ·
        if len(vertices) >= n_points:
            indices = np.random.choice(len(vertices), n_points, replace=False)
        else:
            indices = np.random.choice(len(vertices), n_points, replace=True)
        return vertices[indices]

    try:
        # è½¬æ¢ä¸ºGPU tensor
        vertices_gpu = torch.from_numpy(vertices.astype(np.float32)).to(device)

        # GPUä¸Šçš„éšæœºé‡‡æ ·
        n_vertices = vertices_gpu.shape[0]
        if n_vertices >= n_points:
            indices = torch.randperm(n_vertices, device=device)[:n_points]
        else:
            # é‡å¤é‡‡æ ·
            indices = torch.randint(0, n_vertices, (n_points,), device=device)

        sampled_points = vertices_gpu[indices]

        # è½¬å›CPU numpy
        return sampled_points.cpu().numpy()

    except Exception as e:
        print(f"GPUé‡‡æ ·å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
        # å›é€€åˆ°CPUé‡‡æ ·
        if len(vertices) >= n_points:
            indices = np.random.choice(len(vertices), n_points, replace=False)
        else:
            indices = np.random.choice(len(vertices), n_points, replace=True)
        return vertices[indices]

def gpu_normalize_batch(vertices_list):
    """
    æ‰¹é‡GPUå½’ä¸€åŒ–
    """
    if not USE_GPU or len(vertices_list) == 0:
        return [normalize_mesh_cpu(v) for v in vertices_list]

    try:
        # è®¡ç®—å…¨å±€è¾¹ç•Œæ¡†
        all_vertices = np.concatenate(vertices_list, axis=0)

        # è½¬åˆ°GPU
        vertices_gpu = torch.from_numpy(all_vertices.astype(np.float32)).to(DEVICE)

        # GPUä¸Šè®¡ç®—è¾¹ç•Œæ¡†
        bbox_min = torch.min(vertices_gpu, dim=0)[0]
        bbox_max = torch.max(vertices_gpu, dim=0)[0]

        center = (bbox_min + bbox_max) / 2
        scale = torch.max(bbox_max - bbox_min) / 2

        # æ‰¹é‡å½’ä¸€åŒ–
        normalized_list = []
        start_idx = 0

        for vertices in vertices_list:
            end_idx = start_idx + len(vertices)
            vertices_gpu_batch = torch.from_numpy(vertices.astype(np.float32)).to(DEVICE)
            normalized_gpu = (vertices_gpu_batch - center) / scale
            normalized_list.append(normalized_gpu.cpu().numpy())
            start_idx = end_idx

        return normalized_list

    except Exception as e:
        print(f"GPUæ‰¹é‡å½’ä¸€åŒ–å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
        return [normalize_mesh_cpu(v) for v in vertices_list]

def normalize_mesh_cpu(vertices):
    """CPUç‰ˆæœ¬çš„ç½‘æ ¼å½’ä¸€åŒ–"""
    bbox_min = vertices.min(axis=0)
    bbox_max = vertices.max(axis=0)
    center = (bbox_min + bbox_max) / 2
    scale = np.max(bbox_max - bbox_min) / 2
    return (vertices - center) / scale

def read_mesh_and_labels(obj_path, json_path):
    """è¯»å–meshå’Œå¯¹åº”çš„æ ‡ç­¾"""
    try:
        mesh = trimesh.load(obj_path)
        with open(json_path, 'r', encoding='utf-8') as f:
            label_data = json.load(f)

        labels = []
        for label in label_data['labels']:
            if isinstance(label, (int, float)):
                labels.append(int(label))
            elif isinstance(label, str) and label.isdigit():
                labels.append(int(label))
            else:
                labels.append(0)

        return mesh, np.array(labels)
    except Exception as e:
        print(f"Error reading {obj_path} or {json_path}: {e}")
        return None, None

def get_partial_jaw_by_z_coordinate_fast(vertices, mask_ratio=0.5):
    """
    å¿«é€Ÿç‰ˆæœ¬çš„Zåæ ‡åˆ†å‰²ï¼ˆåªå¤„ç†é¡¶ç‚¹ï¼Œä¸å¤„ç†é¢ï¼‰
    """
    z_min = vertices[:, 2].min()
    z_max = vertices[:, 2].max()
    z_threshold = z_min + (z_max - z_min) * mask_ratio

    # ä¿ç•™Zåæ ‡å¤§äºé˜ˆå€¼çš„ç‚¹
    mask = vertices[:, 2] > z_threshold
    if np.sum(mask) < 100:  # ç¡®ä¿è‡³å°‘æœ‰100ä¸ªç‚¹
        mask = vertices[:, 2] > (z_min + (z_max - z_min) * 0.3)

    return vertices[mask]

def get_partial_jaw_by_random_removal(vertices, exclude_ratio=0.3):
    """
    éšæœºç§»é™¤éƒ¨åˆ†ç‚¹
    """
    n_keep = int(len(vertices) * (1 - exclude_ratio))
    n_keep = max(n_keep, 100)  # è‡³å°‘ä¿ç•™100ä¸ªç‚¹

    indices = np.random.choice(len(vertices), n_keep, replace=False)
    return vertices[indices]

def process_batch_samples(batch_args):
    """
    æ‰¹é‡å¤„ç†æ ·æœ¬ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    """
    batch_data, output_dir, split = batch_args
    processed_samples = []

    try:
        # æ‰¹é‡è¯»å–æ•°æ®
        meshes_data = []
        valid_samples = []

        for sample_path, instance_id in batch_data:
            obj_path = os.path.join(sample_path, "lower.obj")
            json_path = os.path.join(sample_path, "modified_seg.json")

            mesh, labels = read_mesh_and_labels(obj_path, json_path)
            if mesh is not None and labels is not None:
                meshes_data.append((mesh, labels, instance_id, sample_path))
                valid_samples.append(instance_id)

        if not meshes_data:
            return []

        # æ‰¹é‡å½’ä¸€åŒ–
        vertices_list = [mesh.vertices for mesh, _, _, _ in meshes_data]
        normalized_vertices_list = gpu_normalize_batch(vertices_list)

        # å¤„ç†æ¯ä¸ªæ ·æœ¬
        for i, ((mesh, labels, instance_id, sample_path), normalized_vertices) in enumerate(zip(meshes_data, normalized_vertices_list)):
            try:
                # ç”Ÿæˆå®Œæ•´ç‚¹äº‘
                complete_points = gpu_sample_points(normalized_vertices, N_POINTS)

                # ä¿å­˜å®Œæ•´ç‚¹äº‘
                complete_dir = os.path.join(output_dir, split, "complete", "000")
                ensure_dir(complete_dir)
                complete_path = os.path.join(complete_dir, f"{instance_id}.pcd")
                save_pointcloud_fast(complete_points, complete_path)

                # ç”Ÿæˆå¤šä¸ªpartialç‚¹äº‘
                partial_dir = os.path.join(output_dir, split, "partial", "000", instance_id)
                ensure_dir(partial_dir)

                for j in range(N_PARTIAL_VIEWS):
                    # å¿«é€Ÿç”Ÿæˆpartial
                    if random.random() < 0.5:
                        partial_vertices = get_partial_jaw_by_z_coordinate_fast(
                            normalized_vertices, mask_ratio=random.uniform(0.3, 0.7)
                        )
                    else:
                        partial_vertices = get_partial_jaw_by_random_removal(
                            normalized_vertices, exclude_ratio=random.uniform(0.2, 0.5)
                        )

                    # é‡‡æ ·partialç‚¹äº‘
                    partial_points = gpu_sample_points(partial_vertices, N_POINTS // 4)

                    # ä¿å­˜partialç‚¹äº‘
                    partial_path = os.path.join(partial_dir, f"{j:02d}.pcd")
                    save_pointcloud_fast(partial_points, partial_path)

                processed_samples.append(instance_id)

            except Exception as e:
                print(f"Error processing sample {instance_id}: {e}")
                continue

    except Exception as e:
        print(f"Error in batch processing: {e}")

    return processed_samples

def save_pointcloud_fast(points, filepath):
    """å¿«é€Ÿä¿å­˜ç‚¹äº‘"""
    try:
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(points.astype(np.float64))
        o3d.io.write_point_cloud(filepath, pcd)
    except Exception as e:
        # å¤‡ç”¨ä¿å­˜æ–¹æ³•
        np.savetxt(filepath.replace('.pcd', '.txt'), points, fmt='%.6f')

def collect_all_samples():
    """æ”¶é›†æ‰€æœ‰æ ·æœ¬ä¿¡æ¯ï¼Œæ’é™¤æœ‰é—®é¢˜çš„æ ·æœ¬"""
    samples = []
    excluded_count = 0
    total_found = 0

    for jaw_dir in ["lower_jaw1", "lower_jaw2", "lower_jaw3"]:
        jaw_path = os.path.join(RAW_DATA_DIR, jaw_dir)
        if not os.path.exists(jaw_path):
            print(f"Warning: {jaw_path} does not exist")
            continue

        for instance_id in os.listdir(jaw_path):
            instance_path = os.path.join(jaw_path, instance_id)
            if os.path.isdir(instance_path):
                obj_path = os.path.join(instance_path, "lower.obj")
                json_path = os.path.join(instance_path, "modified_seg.json")

                if os.path.exists(obj_path) and os.path.exists(json_path):
                    total_found += 1

                    if is_sample_excluded(instance_id):
                        excluded_count += 1
                        continue

                    samples.append((instance_path, instance_id))

    print(f"ğŸ“Š æ ·æœ¬ç»Ÿè®¡:")
    print(f"   æ€»å‘ç°æ ·æœ¬: {total_found}")
    print(f"   æ’é™¤æ ·æœ¬: {excluded_count}")
    print(f"   æœ‰æ•ˆæ ·æœ¬: {len(samples)}")
    print(f"   æ’é™¤ç‡: {excluded_count/total_found*100:.1f}%" if total_found > 0 else "   æ’é™¤ç‡: 0%")

    return samples

def create_batches(samples, batch_size):
    """å°†æ ·æœ¬åˆ†æ‰¹"""
    batches = []
    for i in range(0, len(samples), batch_size):
        batch = samples[i:i + batch_size]
        batches.append(batch)
    return batches

def create_dataset_metadata(output_dir, train_samples, test_samples):
    """åˆ›å»ºæ•°æ®é›†å…ƒæ•°æ®æ–‡ä»¶"""
    dataset_json = [
        {
            "taxonomy_id": "000",
            "taxonomy_name": "LowerJaw",
            "test": test_samples,
            "train": train_samples,
            "val": []
        }
    ]

    with open(os.path.join(output_dir, "PCN.json"), 'w') as f:
        json.dump(dataset_json, f, indent=2)

    with open(os.path.join(output_dir, "category.txt"), 'w') as f:
        f.write("LowerJaw\n")

    print(f"Dataset metadata saved:")
    print(f"  Train samples: {len(train_samples)}")
    print(f"  Test samples: {len(test_samples)}")

def main():
    """ä¸»å¤„ç†å‡½æ•° - GPUåŠ é€Ÿç‰ˆæœ¬"""
    start_time = time.time()

    print("ğŸš€ å¼€å§‹GPUåŠ é€Ÿç‰™åˆ—æ•°æ®é¢„å¤„ç†...")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    ensure_dir(OUTPUT_DIR)

    # æ”¶é›†æ‰€æœ‰æ ·æœ¬
    print("ğŸ“‹ æ”¶é›†æ ·æœ¬ä¿¡æ¯...")
    all_samples = collect_all_samples()

    if len(all_samples) == 0:
        print("No valid samples found!")
        return

    # éšæœºæ‰“ä¹±å¹¶åˆ†å‰²æ•°æ®
    random.shuffle(all_samples)
    n_train = int(len(all_samples) * TRAIN_RATIO)
    train_samples = all_samples[:n_train]
    test_samples = all_samples[n_train:]

    print(f"ğŸ“Š æ•°æ®åˆ†å‰²: {len(train_samples)} train, {len(test_samples)} test")

    # åˆ›å»ºæ‰¹æ¬¡
    train_batches = create_batches(train_samples, BATCH_SIZE)
    test_batches = create_batches(test_samples, BATCH_SIZE)

    print(f"ğŸ”„ æ‰¹å¤„ç†é…ç½®: {len(train_batches)} train batches, {len(test_batches)} test batches")

    # å¤„ç†è®­ç»ƒé›†
    print("âš¡ å¤„ç†è®­ç»ƒé›†...")
    processed_train = []

    with tqdm(total=len(train_batches), desc="Training batches") as pbar:
        for batch in train_batches:
            batch_result = process_batch_samples((batch, OUTPUT_DIR, "train"))
            processed_train.extend(batch_result)
            pbar.update(1)

    # å¤„ç†æµ‹è¯•é›†
    print("âš¡ å¤„ç†æµ‹è¯•é›†...")
    processed_test = []

    with tqdm(total=len(test_batches), desc="Testing batches") as pbar:
        for batch in test_batches:
            batch_result = process_batch_samples((batch, OUTPUT_DIR, "test"))
            processed_test.extend(batch_result)
            pbar.update(1)

    # åˆ›å»ºæ•°æ®é›†å…ƒæ•°æ®
    print("ğŸ“ åˆ›å»ºæ•°æ®é›†å…ƒæ•°æ®...")
    create_dataset_metadata(OUTPUT_DIR, processed_train, processed_test)

    end_time = time.time()
    total_time = end_time - start_time

    print(f"\nğŸ‰ GPUåŠ é€Ÿé¢„å¤„ç†å®Œæˆï¼")
    print(f"â±ï¸  æ€»å¤„ç†æ—¶é—´: {total_time:.2f} ç§’")
    print(f"ğŸ“Š å¤„ç†é€Ÿåº¦: {len(all_samples)/total_time:.2f} æ ·æœ¬/ç§’")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"âœ… æˆåŠŸå¤„ç†: {len(processed_train)} train + {len(processed_test)} test = {len(processed_train) + len(processed_test)} æ ·æœ¬")

if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    np.random.seed(42)
    if USE_GPU:
        torch.manual_seed(42)
        torch.cuda.manual_seed_all(42)

    main()