#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‰™åˆ—æ•°æ®é¢„å¤„ç†è„šæœ¬ for PointAttN
å‚è€ƒPoinTRçš„å¤„ç†æ–¹å¼ï¼Œä¸ºPointAttNæ¨¡å‹å‡†å¤‡æ•°æ®

ä¸»è¦åŠŸèƒ½ï¼š
1. å¤„ç†data_prepareä¸­çš„lower_jawæ•°æ®
2. å®ç°ä¸‹åŠéƒ¨åˆ†ç‰™é½¿maskï¼ˆç§»é™¤ä¸‹åŠéƒ¨åˆ†è®©æ¨¡å‹é¢„æµ‹ï¼‰
3. åˆ†å‰²train/testæ•°æ®é›†
4. ç”ŸæˆPointAttNå…¼å®¹çš„æ•°æ®æ ¼å¼
"""

import os
import json
import trimesh
import numpy as np
import open3d as o3d
from tqdm import tqdm
import random
import shutil
from pathlib import Path
import multiprocessing
from functools import partial
from excluded_samples import EXCLUDED_SAMPLE_IDS, is_sample_excluded

# é…ç½®å‚æ•°
RAW_DATA_DIR = "data_prepare/é‡æ ‡æ³¨åå®Œæ•´ç‰™åˆ—lower_jaw"
OUTPUT_DIR = "processed_jaw_data"
N_POINTS = 2048  # PointAttNä½¿ç”¨çš„ç‚¹æ•°
TRAIN_RATIO = 0.8
TEST_RATIO = 0.2
N_PARTIAL_VIEWS = 8  # æ¯ä¸ªcompleteå¯¹åº”8ä¸ªpartial

# ä¸‹é¢Œç‰™é½¿FDIç¼–å·ï¼ˆç”¨äºç¡®å®šä¸‹åŠéƒ¨åˆ†ï¼‰
# FDIç¼–å·ä¸­ï¼Œä¸‹é¢Œç‰™é½¿çš„ç¼–å·
LOWER_JAW_TEETH = [31, 32, 33, 34, 35, 36, 37, 38, 41, 42, 43, 44, 45, 46, 47, 48]

def ensure_dir(path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    Path(path).mkdir(parents=True, exist_ok=True)

def read_mesh_and_labels(obj_path, json_path):
    """
    è¯»å–meshå’Œå¯¹åº”çš„æ ‡ç­¾
    """
    try:
        # è¯»å–mesh
        mesh = trimesh.load(obj_path)

        # è¯»å–æ ‡ç­¾
        with open(json_path, 'r', encoding='utf-8') as f:
            label_data = json.load(f)

        # å¤„ç†æ ‡ç­¾ï¼Œç¡®ä¿æ˜¯æ•°å€¼å‹
        labels = []
        for label in label_data['labels']:
            if isinstance(label, (int, float)):
                labels.append(int(label))
            elif isinstance(label, str) and label.isdigit():
                labels.append(int(label))
            else:
                labels.append(0)

        return mesh, np.array(labels)
    except Exception as e:
        print(f"Error reading {obj_path} or {json_path}: {e}")
        return None, None

def get_partial_jaw_by_z_coordinate(mesh, labels, mask_ratio=0.5):
    """
    æ ¹æ®Zåæ ‡ç§»é™¤ä¸‹åŠéƒ¨åˆ†ç‰™é½¿ï¼ˆæ¨¡æ‹Ÿmaskä¸‹åŠéƒ¨åˆ†ï¼‰
    mask_ratio: éœ€è¦ç§»é™¤çš„ä¸‹åŠéƒ¨åˆ†æ¯”ä¾‹ (0.5è¡¨ç¤ºç§»é™¤ä¸‹åŠ50%)
    """
    vertices = mesh.vertices

    # è®¡ç®—Zåæ ‡çš„èŒƒå›´
    z_min = vertices[:, 2].min()
    z_max = vertices[:, 2].max()

    # è®¡ç®—åˆ†å‰²çº¿ï¼šç§»é™¤ä¸‹åŠéƒ¨åˆ†
    z_threshold = z_min + (z_max - z_min) * mask_ratio

    # æ‰¾å‡ºéœ€è¦ä¿ç•™çš„é¢ï¼ˆè‡³å°‘æœ‰ä¸€ä¸ªé¡¶ç‚¹åœ¨åˆ†å‰²çº¿ä»¥ä¸Šï¼‰
    valid_faces = []
    for i, face in enumerate(mesh.faces):
        # æ£€æŸ¥è¿™ä¸ªé¢çš„ä¸‰ä¸ªé¡¶ç‚¹æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªåœ¨åˆ†å‰²çº¿ä»¥ä¸Š
        face_vertices = vertices[face]
        if np.any(face_vertices[:, 2] > z_threshold):
            valid_faces.append(i)

    if len(valid_faces) == 0:
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆé¢ï¼Œè¿”å›åŸmeshçš„ä¸€ä¸ªå°éƒ¨åˆ†
        valid_faces = list(range(min(100, len(mesh.faces))))

    # åˆ›å»ºæ–°çš„mesh
    try:
        partial_mesh = mesh.submesh([valid_faces], append=True)
        return partial_mesh
    except:
        # å¦‚æœsubmeshå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„partial mesh
        return mesh

def get_partial_jaw_by_teeth_removal(mesh, labels, exclude_ratio=0.3):
    """
    é€šè¿‡éšæœºç§»é™¤ä¸€äº›ä¸‹é¢Œç‰™é½¿æ¥åˆ›å»ºpartial jaw
    exclude_ratio: è¦ç§»é™¤çš„ç‰™é½¿æ¯”ä¾‹
    """
    # æ‰¾å‡ºæ‰€æœ‰å­˜åœ¨çš„ç‰™é½¿æ ‡ç­¾
    unique_labels = np.unique(labels)
    existing_teeth = [label for label in unique_labels if label > 0]

    if len(existing_teeth) == 0:
        return mesh

    # éšæœºé€‰æ‹©è¦ç§»é™¤çš„ç‰™é½¿
    n_exclude = max(1, int(len(existing_teeth) * exclude_ratio))
    exclude_teeth = random.sample(existing_teeth, n_exclude)

    # æ‰¾å‡ºéœ€è¦ä¿ç•™çš„é¢
    valid_faces = []
    for i, face in enumerate(mesh.faces):
        # æ£€æŸ¥è¿™ä¸ªé¢çš„ä¸‰ä¸ªé¡¶ç‚¹å¯¹åº”çš„æ ‡ç­¾
        face_labels = labels[face]
        # å¦‚æœé¢çš„ä»»ä¸€é¡¶ç‚¹å±äºè¦ç§»é™¤çš„ç‰™é½¿ï¼Œåˆ™ç§»é™¤è¿™ä¸ªé¢
        if not np.any(np.isin(face_labels, exclude_teeth)):
            valid_faces.append(i)

    if len(valid_faces) == 0:
        valid_faces = list(range(min(100, len(mesh.faces))))

    try:
        partial_mesh = mesh.submesh([valid_faces], append=True)
        return partial_mesh
    except:
        return mesh

def normalize_mesh(mesh):
    """
    å½’ä¸€åŒ–meshåˆ°[-0.5, 0.5]^3
    """
    vertices = mesh.vertices

    # è®¡ç®—è¾¹ç•Œæ¡†
    bbox_min = vertices.min(axis=0)
    bbox_max = vertices.max(axis=0)

    # è®¡ç®—ä¸­å¿ƒå’Œç¼©æ”¾æ¯”ä¾‹
    center = (bbox_min + bbox_max) / 2
    scale = np.max(bbox_max - bbox_min)

    # å½’ä¸€åŒ–
    mesh.vertices = (vertices - center) / scale

    return mesh

def mesh_to_pointcloud(mesh, n_points=N_POINTS):
    """
    å°†meshè½¬æ¢ä¸ºç‚¹äº‘
    """
    try:
        # ä½¿ç”¨trimeshé‡‡æ ·
        if hasattr(mesh, 'sample'):
            points = mesh.sample(n_points)
        else:
            # å¤‡é€‰æ–¹æ³•ï¼šå‡åŒ€é‡‡æ ·é¡¶ç‚¹
            vertices = mesh.vertices
            if len(vertices) >= n_points:
                indices = np.random.choice(len(vertices), n_points, replace=False)
            else:
                # å¦‚æœé¡¶ç‚¹æ•°ä¸å¤Ÿï¼Œè¿›è¡Œé‡å¤é‡‡æ ·
                indices = np.random.choice(len(vertices), n_points, replace=True)
            points = vertices[indices]

        return points
    except Exception as e:
        print(f"Error sampling mesh: {e}")
        # è¿”å›éšæœºç‚¹ä½œä¸ºå¤‡é€‰
        return np.random.randn(n_points, 3) * 0.1

def save_pointcloud(points, filepath):
    """
    ä¿å­˜ç‚¹äº‘ä¸ºPCDæ ¼å¼
    """
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(points)
    o3d.io.write_point_cloud(filepath, pcd)

def process_single_sample(args):
    """
    å¤„ç†å•ä¸ªæ ·æœ¬
    """
    sample_dir, instance_id, output_dir, split = args

    try:
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        obj_path = os.path.join(sample_dir, "lower.obj")
        json_path = os.path.join(sample_dir, "modified_seg.json")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(obj_path) or not os.path.exists(json_path):
            print(f"Missing files for {instance_id}")
            return None

        # è¯»å–æ•°æ®
        mesh, labels = read_mesh_and_labels(obj_path, json_path)
        if mesh is None or labels is None:
            return None

        # å½’ä¸€åŒ–mesh
        mesh = normalize_mesh(mesh)

        # ç”Ÿæˆå®Œæ•´ç‚¹äº‘
        complete_points = mesh_to_pointcloud(mesh, N_POINTS)

        # ä¿å­˜å®Œæ•´ç‚¹äº‘
        complete_dir = os.path.join(output_dir, split, "complete", "000")  # 000æ˜¯lower jawçš„ç±»åˆ«ID
        ensure_dir(complete_dir)
        complete_path = os.path.join(complete_dir, f"{instance_id}.pcd")
        save_pointcloud(complete_points, complete_path)

        # ç”Ÿæˆå¤šä¸ªpartialç‚¹äº‘
        partial_dir = os.path.join(output_dir, split, "partial", "000", instance_id)
        ensure_dir(partial_dir)

        for i in range(N_PARTIAL_VIEWS):
            # éšæœºé€‰æ‹©partialç”Ÿæˆæ–¹å¼
            if random.random() < 0.5:
                # ä½¿ç”¨Zåæ ‡ç§»é™¤ä¸‹åŠéƒ¨åˆ†
                partial_mesh = get_partial_jaw_by_z_coordinate(
                    mesh, labels, mask_ratio=random.uniform(0.3, 0.7)
                )
            else:
                # ä½¿ç”¨ç‰™é½¿ç§»é™¤æ–¹å¼
                partial_mesh = get_partial_jaw_by_teeth_removal(
                    mesh, labels, exclude_ratio=random.uniform(0.2, 0.5)
                )

            # ç”Ÿæˆpartialç‚¹äº‘
            partial_points = mesh_to_pointcloud(partial_mesh, N_POINTS // 4)

            # ä¿å­˜partialç‚¹äº‘
            partial_path = os.path.join(partial_dir, f"{i:02d}.pcd")
            save_pointcloud(partial_points, partial_path)

        return instance_id

    except Exception as e:
        print(f"Error processing {instance_id}: {e}")
        return None

def collect_all_samples():
    """
    æ”¶é›†æ‰€æœ‰æ ·æœ¬ä¿¡æ¯ï¼Œæ’é™¤æœ‰é—®é¢˜çš„æ ·æœ¬
    """
    samples = []
    excluded_count = 0
    total_found = 0

    for jaw_dir in ["lower_jaw1", "lower_jaw2", "lower_jaw3"]:
        jaw_path = os.path.join(RAW_DATA_DIR, jaw_dir)
        if not os.path.exists(jaw_path):
            print(f"Warning: {jaw_path} does not exist")
            continue

        for instance_id in os.listdir(jaw_path):
            instance_path = os.path.join(jaw_path, instance_id)
            if os.path.isdir(instance_path):
                # æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                obj_path = os.path.join(instance_path, "lower.obj")
                json_path = os.path.join(instance_path, "modified_seg.json")

                if os.path.exists(obj_path) and os.path.exists(json_path):
                    total_found += 1

                    # æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­
                    if is_sample_excluded(instance_id):
                        excluded_count += 1
                        print(f"âŒ æ’é™¤æ ·æœ¬: {instance_id}")
                        continue

                    samples.append((instance_path, instance_id))

    print(f"ğŸ“Š æ ·æœ¬ç»Ÿè®¡:")
    print(f"   æ€»å‘ç°æ ·æœ¬: {total_found}")
    print(f"   æ’é™¤æ ·æœ¬: {excluded_count}")
    print(f"   æœ‰æ•ˆæ ·æœ¬: {len(samples)}")
    print(f"   æ’é™¤ç‡: {excluded_count/total_found*100:.1f}%" if total_found > 0 else "   æ’é™¤ç‡: 0%")

    return samples

def create_dataset_metadata(output_dir, train_samples, test_samples):
    """
    åˆ›å»ºæ•°æ®é›†å…ƒæ•°æ®æ–‡ä»¶
    """
    # åˆ›å»ºPCNæ ¼å¼çš„jsonæ–‡ä»¶
    dataset_json = [
        {
            "taxonomy_id": "000",
            "taxonomy_name": "LowerJaw",
            "test": test_samples,
            "train": train_samples,
            "val": []  # æˆ‘ä»¬åªç”¨trainå’Œtest
        }
    ]

    with open(os.path.join(output_dir, "PCN.json"), 'w') as f:
        json.dump(dataset_json, f, indent=2)

    # åˆ›å»ºç±»åˆ«æ–‡ä»¶
    with open(os.path.join(output_dir, "category.txt"), 'w') as f:
        f.write("LowerJaw\n")

    print(f"Dataset metadata saved:")
    print(f"  Train samples: {len(train_samples)}")
    print(f"  Test samples: {len(test_samples)}")

def main():
    """
    ä¸»å¤„ç†å‡½æ•°
    """
    print("å¼€å§‹å¤„ç†ç‰™åˆ—æ•°æ®...")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    ensure_dir(OUTPUT_DIR)

    # æ”¶é›†æ‰€æœ‰æ ·æœ¬
    print("æ”¶é›†æ ·æœ¬ä¿¡æ¯...")
    all_samples = collect_all_samples()

    if len(all_samples) == 0:
        print("No valid samples found!")
        return

    # éšæœºæ‰“ä¹±å¹¶åˆ†å‰²æ•°æ®
    random.shuffle(all_samples)
    n_train = int(len(all_samples) * TRAIN_RATIO)
    train_samples = all_samples[:n_train]
    test_samples = all_samples[n_train:]

    print(f"æ•°æ®åˆ†å‰²: {len(train_samples)} train, {len(test_samples)} test")

    # å‡†å¤‡å¤„ç†ä»»åŠ¡
    tasks = []

    # è®­ç»ƒé›†ä»»åŠ¡
    for sample_path, instance_id in train_samples:
        tasks.append((sample_path, instance_id, OUTPUT_DIR, "train"))

    # æµ‹è¯•é›†ä»»åŠ¡
    for sample_path, instance_id in test_samples:
        tasks.append((sample_path, instance_id, OUTPUT_DIR, "test"))

    # å¹¶è¡Œå¤„ç†
    print("å¼€å§‹å¤„ç†æ ·æœ¬...")
    num_processes = min(8, len(tasks))  # ä½¿ç”¨8ä¸ªè¿›ç¨‹æˆ–ä»»åŠ¡æ•°

    processed_train = []
    processed_test = []

    with multiprocessing.Pool(processes=num_processes) as pool:
        with tqdm(total=len(tasks), desc="Processing samples") as pbar:
            results = pool.imap_unordered(process_single_sample, tasks)

            for i, result in enumerate(results):
                if result is not None:
                    # ç¡®å®šè¿™ä¸ªæ ·æœ¬å±äºå“ªä¸ªsplit
                    if i < len(train_samples):
                        processed_train.append(result)
                    else:
                        processed_test.append(result)
                pbar.update(1)

    # åˆ›å»ºæ•°æ®é›†å…ƒæ•°æ®
    print("åˆ›å»ºæ•°æ®é›†å…ƒæ•°æ®...")
    create_dataset_metadata(OUTPUT_DIR, processed_train, processed_test)

    print(f"æ•°æ®å¤„ç†å®Œæˆï¼")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"æˆåŠŸå¤„ç†: {len(processed_train)} train + {len(processed_test)} test = {len(processed_train) + len(processed_test)} samples")

    # æ˜¾ç¤ºç›®å½•ç»“æ„
    print(f"\nç”Ÿæˆçš„ç›®å½•ç»“æ„:")
    print(f"{OUTPUT_DIR}/")
    print(f"  â”œâ”€â”€ train/")
    print(f"  â”‚   â”œâ”€â”€ complete/000/")
    print(f"  â”‚   â””â”€â”€ partial/000/")
    print(f"  â”œâ”€â”€ test/")
    print(f"  â”‚   â”œâ”€â”€ complete/000/")
    print(f"  â”‚   â””â”€â”€ partial/000/")
    print(f"  â”œâ”€â”€ PCN.json")
    print(f"  â””â”€â”€ category.txt")

if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    np.random.seed(42)

    main()